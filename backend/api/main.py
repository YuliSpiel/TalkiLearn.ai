from fastapi import FastAPI, UploadFile, File, HTTPException, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from typing import List, Optional, Generator
import os
import uuid
from datetime import datetime
import json
import time

from ..models import (
    Notebook,
    Session,
    Subsession,
    UserProfile,
    SessionStatus,
    ChatRequest,
    ChatResponse,
    QuizGenerateRequest,
    QuizGenerateResponse,
    QuizSubmitRequest,
    QuizSubmitResponse,
    QuizResult,
    SummaryRequest,
    Summary,
    CompleteSubsessionRequest,
)
from ..services import (
    VectorStoreService,
    EmbeddingService,
    LLMService,
    DocumentProcessor,
    get_embedding_service,
    get_llm_service,
)
from ..utils import get_database

# FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI(
    title="TalkiLearn API",
    description="í•™ìŠµë³´ì¡° ì±—ë´‡ ë°±ì—”ë“œ API",
    version="1.0.0"
)

# CORS ì„¤ì • (Streamlit í”„ë¡ íŠ¸ì—”ë“œì™€ í†µì‹ )
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ê°œë°œìš© - í”„ë¡œë•ì…˜ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ (ì „ì—­)
vector_store = VectorStoreService()
db = get_database()  # Database ì‹±ê¸€í†¤ ì œê±°ë¡œ ë§¤ë²ˆ ìµœì‹  ë°ì´í„° ì½ìŒ


@app.on_event("startup")
def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
    print("ğŸš€ TalkiLearn API starting up...")
    print("ğŸ“¦ Loading embedding model...")
    # ì„ë² ë”© ëª¨ë¸ ì‚¬ì „ ë¡œë“œ
    get_embedding_service()
    print("âœ… Embedding model loaded successfully")
    print("ğŸ¤– LLM service initialized")
    get_llm_service()
    print("âœ… TalkiLearn API ready!")


# ========== Health Check ==========

@app.get("/")
def root():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "service": "TalkiLearn API",
        "version": "1.0.0"
    }


# ========== 1. Notebooks ==========

@app.post("/notebooks", response_model=Notebook)
def create_notebook(title: str, user_id: str = "default_user"):
    """
    ë…¸íŠ¸ë¶ ìƒì„±

    Args:
        title: ê³¼ëª©ëª…
        user_id: ì‚¬ìš©ì ID
    """
    notebook = db.create_notebook(title=title, user_id=user_id)
    return notebook


@app.get("/notebooks", response_model=List[Notebook])
def get_notebooks(user_id: str = "default_user"):
    """
    ë…¸íŠ¸ë¶ ëª©ë¡ ì¡°íšŒ

    Args:
        user_id: ì‚¬ìš©ì ID
    """
    notebooks = db.get_all_notebooks(user_id=user_id)
    return notebooks


@app.get("/notebooks/{notebook_id}", response_model=Notebook)
def get_notebook(notebook_id: int):
    """
    ë…¸íŠ¸ë¶ ìƒì„¸ ì¡°íšŒ

    Args:
        notebook_id: ë…¸íŠ¸ë¶ ID
    """
    notebook = db.get_notebook(notebook_id)
    if not notebook:
        raise HTTPException(status_code=404, detail="Notebook not found")
    return notebook


@app.delete("/notebooks/{notebook_id}")
def delete_notebook(notebook_id: int):
    """
    ë…¸íŠ¸ë¶ ì‚­ì œ

    Args:
        notebook_id: ë…¸íŠ¸ë¶ ID
    """
    notebook = db.get_notebook(notebook_id)
    if not notebook:
        raise HTTPException(status_code=404, detail="Notebook not found")

    # ë…¸íŠ¸ë¶ì˜ ëª¨ë“  ì„¸ì…˜ì— ëŒ€í•œ ë²¡í„° ìŠ¤í† ì–´ ì²­í¬ ì‚­ì œ
    for session in notebook.sessions:
        try:
            vector_store.delete_session_chunks(session.session_id)
        except Exception as e:
            print(f"Warning: Failed to delete chunks for session {session.session_id}: {e}")

    # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë…¸íŠ¸ë¶ ì‚­ì œ
    success = db.delete_notebook(notebook_id)
    if not success:
        raise HTTPException(status_code=500, detail="Failed to delete notebook")

    return {"message": "Notebook deleted successfully", "notebook_id": notebook_id}


# ========== 2. Session Upload ==========

@app.post("/sessions:upload")
async def upload_session(
    notebook_id: int = Form(...),
    file: UploadFile = File(...)
):
    """
    íŒŒì¼ ì—…ë¡œë“œ ë° ì„¸ì…˜ ìƒì„±

    1. íŒŒì¼ íŒŒì‹± (txt/pdf)
    2. ì²­í‚¹ (size=800, overlap=0.2)
    3. ì„ë² ë”© ìƒì„±
    4. Chromaì— ì €ì¥
    5. í† í”½ í´ëŸ¬ìŠ¤í„°ë§ â†’ Subsession ë¶„í• 
    6. ì„¸ì…˜ ìƒíƒœë¥¼ indexedë¡œ ë³€ê²½

    Args:
        notebook_id: ë…¸íŠ¸ë¶ ID
        file: ì—…ë¡œë“œ íŒŒì¼
    """
    print(f"ğŸ“¥ Upload request received! notebook_id={notebook_id}, filename={file.filename}")

    # ë…¸íŠ¸ë¶ ì¡´ì¬ í™•ì¸
    notebook = db.get_notebook(notebook_id)
    if not notebook:
        raise HTTPException(status_code=404, detail="Notebook not found")

    # íŒŒì¼ íƒ€ì… í™•ì¸
    filename = file.filename
    file_type = filename.split(".")[-1].lower()
    if file_type not in ["txt", "pdf"]:
        raise HTTPException(status_code=400, detail="Unsupported file type. Only txt and pdf are allowed.")

    # ì„¸ì…˜ ìƒì„±
    max_session_id = max([s.session_id for s in notebook.sessions], default=0)
    new_session_id = max_session_id + 1

    session = Session(
        session_id=new_session_id,
        notebook_id=notebook_id,
        filename=filename,
        file_type=file_type,
        status=SessionStatus.PROCESSING
    )

    db.add_session_to_notebook(notebook_id, session)

    try:
        # 1. íŒŒì¼ ì½ê¸° (ë¹„ë™ê¸° ë°©ì‹)
        file_bytes = await file.read()

        # 2. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        from ..services.document_processor import extract_text_from_bytes
        text = extract_text_from_bytes(file_bytes, file_type)

        # 3. ì²­í‚¹
        processor = DocumentProcessor(chunk_size=800, overlap_ratio=0.2)
        chunks = processor.chunk_text(text)
        session.total_chunks = len(chunks)

        # 4. ì„ë² ë”© ìƒì„±
        embedding_service = get_embedding_service()
        embeddings = embedding_service.encode(chunks, show_progress=False)

        # 5. í† í”½ í´ëŸ¬ìŠ¤í„°ë§
        cluster_labels, num_clusters = processor.cluster_chunks_into_subsessions(
            embeddings,
            min_clusters=3,
            max_clusters=8
        )

        # 6. Subsession ìƒì„±
        subsessions = []
        # ê³ ìœ  ID ìƒì„±: notebook_idì™€ session_idë¥¼ ì¡°í•©í•˜ì—¬ ì „ì—­ì ìœ¼ë¡œ ê³ ìœ í•˜ê²Œ ë§Œë“¦
        subsession_id_base = notebook_id * 100000 + new_session_id * 1000

        for cluster_idx in range(num_clusters):
            # í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì˜ ì²­í¬ ì¸ë±ìŠ¤ ì°¾ê¸°
            cluster_chunk_indices = [i for i, label in enumerate(cluster_labels) if label == cluster_idx]

            # ì²­í¬ ID ë¦¬ìŠ¤íŠ¸
            chunk_ids = list(range(subsession_id_base + cluster_idx * 100, subsession_id_base + cluster_idx * 100 + len(cluster_chunk_indices)))

            # ì„œë¸Œì„¸ì…˜ ì œëª© ìƒì„± (LLM ì‚¬ìš©)
            cluster_chunks = [chunks[i] for i in cluster_chunk_indices]
            llm_service = get_llm_service()
            title = llm_service.generate_subsession_title(cluster_chunks)

            subsession = Subsession(
                subsession_id=subsession_id_base + cluster_idx,
                session_id=new_session_id,
                index=cluster_idx + 1,
                title=title,
                chunk_ids=chunk_ids
            )
            subsessions.append(subsession)

            # 7. Chromaì— ì²­í¬ ì €ì¥
            chunk_embeddings = [embeddings[i] for i in cluster_chunk_indices]
            chunk_documents = cluster_chunks
            chunk_metadatas = [
                {
                    "notebook_id": notebook_id,
                    "session_id": new_session_id,
                    "subsession_id": subsession.subsession_id,
                    "chunk_id": chunk_id,
                    "cluster": cluster_idx
                }
                for chunk_id in chunk_ids
            ]
            chunk_id_strings = [str(chunk_id) for chunk_id in chunk_ids]

            vector_store.add_chunks(
                chunks=chunk_documents,
                embeddings=chunk_embeddings,
                metadatas=chunk_metadatas,
                chunk_ids=chunk_id_strings
            )

        # 8. ì„¸ì…˜ ì—…ë°ì´íŠ¸
        session.subsessions = subsessions
        session.status = SessionStatus.INDEXED
        session.indexed_at = datetime.now()

        db.update_session(notebook_id, session)

        return {
            "message": "File uploaded and processed successfully",
            "session_id": new_session_id,
            "total_chunks": len(chunks),
            "num_subsessions": num_clusters,
            "subsessions": [
                {
                    "subsession_id": sub.subsession_id,
                    "index": sub.index,
                    "title": sub.title,
                    "num_chunks": len(sub.chunk_ids)
                }
                for sub in subsessions
            ]
        }

    except Exception as e:
        # ì—ëŸ¬ ë°œìƒ ì‹œ ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        session.status = SessionStatus.ERROR
        db.update_session(notebook_id, session)
        raise HTTPException(status_code=500, detail=f"Error processing file: {str(e)}")


@app.post("/sessions:upload-stream")
def upload_session_with_progress(
    notebook_id: int = Form(...),
    file: UploadFile = File(...)
):
    """
    íŒŒì¼ ì—…ë¡œë“œ ë° ì„¸ì…˜ ìƒì„± (ì§„í–‰ë¥  ìŠ¤íŠ¸ë¦¬ë°)

    Server-Sent Events (SSE) í˜•ì‹ìœ¼ë¡œ ì§„í–‰ë¥ ì„ ì‹¤ì‹œê°„ ì „ì†¡í•©ë‹ˆë‹¤.
    """
    def generate_progress() -> Generator[str, None, None]:
        try:
            # ë…¸íŠ¸ë¶ ì¡´ì¬ í™•ì¸
            notebook = db.get_notebook(notebook_id)
            if not notebook:
                yield f"data: {json.dumps({'error': 'Notebook not found'})}\n\n"
                return

            # íŒŒì¼ íƒ€ì… í™•ì¸
            filename = file.filename
            file_type = filename.split(".")[-1].lower()
            if file_type not in ["txt", "pdf"]:
                yield f"data: {json.dumps({'error': 'Unsupported file type'})}\n\n"
                return

            yield f"data: {json.dumps({'stage': 'init', 'message': 'íŒŒì¼ ì—…ë¡œë“œ ì‹œì‘...', 'progress': 0})}\n\n"
            time.sleep(0.1)

            # ì„¸ì…˜ ìƒì„±
            max_session_id = max([s.session_id for s in notebook.sessions], default=0)
            new_session_id = max_session_id + 1

            session = Session(
                session_id=new_session_id,
                notebook_id=notebook_id,
                filename=filename,
                file_type=file_type,
                status=SessionStatus.PROCESSING
            )
            db.add_session_to_notebook(notebook_id, session)

            # 1. íŒŒì¼ ì½ê¸°
            yield f"data: {json.dumps({'stage': 'reading', 'message': 'íŒŒì¼ ì½ëŠ” ì¤‘...', 'progress': 10})}\n\n"
            file_bytes = file.file.read()

            # 2. í…ìŠ¤íŠ¸ ì¶”ì¶œ
            yield f"data: {json.dumps({'stage': 'extracting', 'message': 'í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...', 'progress': 20})}\n\n"
            from ..services.document_processor import extract_text_from_bytes
            text = extract_text_from_bytes(file_bytes, file_type)

            # 3. ì²­í‚¹
            yield f"data: {json.dumps({'stage': 'chunking', 'message': 'í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘...', 'progress': 30})}\n\n"
            processor = DocumentProcessor(chunk_size=800, overlap_ratio=0.2)
            chunks = processor.chunk_text(text)
            session.total_chunks = len(chunks)

            # 4. ì„ë² ë”© ìƒì„± (ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ë©° ì§„í–‰ë¥  ì „ì†¡)
            embedding_service = get_embedding_service()
            embeddings = []
            batch_size = 64
            total_chunks = len(chunks)

            yield f"data: {json.dumps({'stage': 'embedding', 'message': f'ì„ë² ë”© ìƒì„± ì¤‘ (0/{total_chunks})...', 'progress': 30})}\n\n"
            time.sleep(0.01)  # ì—°ê²° ìœ ì§€

            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ë©° ê° ë°°ì¹˜ë§ˆë‹¤ ì§„í–‰ë¥  ì „ì†¡
            for i in range(0, total_chunks, batch_size):
                batch = chunks[i:i + batch_size]
                batch_embeddings = embedding_service.encode(batch, show_progress=False)
                embeddings.extend(batch_embeddings)

                # ì§„í–‰ë¥  ê³„ì‚° ë° ì „ì†¡
                processed = min(i + batch_size, total_chunks)
                progress = 30 + int((processed / total_chunks) * 40)  # 30-70%
                yield f"data: {json.dumps({'stage': 'embedding', 'message': f'ì„ë² ë”© ìƒì„± ì¤‘ ({processed}/{total_chunks})...', 'progress': progress})}\n\n"
                time.sleep(0.01)  # ì—°ê²° ìœ ì§€ ë° CPU ì–‘ë³´

            yield f"data: {json.dumps({'stage': 'embedding', 'message': f'ì„ë² ë”© ìƒì„± ì™„ë£Œ ({total_chunks}/{total_chunks})', 'progress': 70})}\n\n"

            # 5. í† í”½ í´ëŸ¬ìŠ¤í„°ë§
            yield f"data: {json.dumps({'stage': 'clustering', 'message': 'í† í”½ í´ëŸ¬ìŠ¤í„°ë§ ì¤‘...', 'progress': 75})}\n\n"
            time.sleep(0.01)  # ì—°ê²° ìœ ì§€

            cluster_labels, num_clusters = processor.cluster_chunks_into_subsessions(
                embeddings,
                min_clusters=3,
                max_clusters=8
            )

            # 6. Subsession ìƒì„±
            yield f"data: {json.dumps({'stage': 'creating_subsessions', 'message': 'ì„œë¸Œì„¸ì…˜ ìƒì„± ì¤‘...', 'progress': 80})}\n\n"
            time.sleep(0.01)  # ì—°ê²° ìœ ì§€

            subsessions = []
            subsession_id_base = notebook_id * 100000 + new_session_id * 1000

            for cluster_idx in range(num_clusters):
                cluster_chunk_indices = [i for i, label in enumerate(cluster_labels) if label == cluster_idx]
                chunk_ids = list(range(subsession_id_base + cluster_idx * 100, subsession_id_base + cluster_idx * 100 + len(cluster_chunk_indices)))

                # ì„œë¸Œì„¸ì…˜ ì œëª© ìƒì„±
                cluster_chunks = [chunks[i] for i in cluster_chunk_indices]
                llm_service = get_llm_service()
                title = llm_service.generate_subsession_title(cluster_chunks)

                subsession = Subsession(
                    subsession_id=subsession_id_base + cluster_idx,
                    session_id=new_session_id,
                    index=cluster_idx + 1,
                    title=title,
                    chunk_ids=chunk_ids
                )
                subsessions.append(subsession)

                # 7. Chromaì— ì²­í¬ ì €ì¥
                chunk_embeddings = [embeddings[i] for i in cluster_chunk_indices]
                chunk_documents = cluster_chunks
                chunk_metadatas = [
                    {
                        "notebook_id": notebook_id,
                        "session_id": new_session_id,
                        "subsession_id": subsession.subsession_id,
                        "chunk_id": chunk_id,
                        "cluster": cluster_idx
                    }
                    for chunk_id in chunk_ids
                ]
                chunk_id_strings = [str(chunk_id) for chunk_id in chunk_ids]

                vector_store.add_chunks(
                    chunks=chunk_documents,
                    embeddings=chunk_embeddings,
                    metadatas=chunk_metadatas,
                    chunk_ids=chunk_id_strings
                )

                progress = 80 + int(((cluster_idx + 1) / num_clusters) * 15)  # 80-95%
                yield f"data: {json.dumps({'stage': 'saving', 'message': f'ì„œë¸Œì„¸ì…˜ ì €ì¥ ì¤‘ ({cluster_idx+1}/{num_clusters})...', 'progress': progress})}\n\n"
                time.sleep(0.01)  # ì—°ê²° ìœ ì§€

            # 8. ì„¸ì…˜ ì—…ë°ì´íŠ¸
            session.subsessions = subsessions
            session.status = SessionStatus.INDEXED
            session.indexed_at = datetime.now()
            db.update_session(notebook_id, session)

            # ì™„ë£Œ
            result = {
                "stage": "complete",
                "message": "íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!",
                "progress": 100,
                "session_id": new_session_id,
                "total_chunks": len(chunks),
                "num_subsessions": num_clusters,
                "subsessions": [
                    {
                        "subsession_id": sub.subsession_id,
                        "index": sub.index,
                        "title": sub.title,
                        "num_chunks": len(sub.chunk_ids)
                    }
                    for sub in subsessions
                ]
            }
            yield f"data: {json.dumps(result)}\n\n"

        except Exception as e:
            # ì—ëŸ¬ ë°œìƒ ì‹œ
            if 'session' in locals():
                session.status = SessionStatus.ERROR
                db.update_session(notebook_id, session)

            error_data = {
                "stage": "error",
                "message": f"ì—ëŸ¬ ë°œìƒ: {str(e)}",
                "progress": 0
            }
            yield f"data: {json.dumps(error_data)}\n\n"

    return StreamingResponse(
        generate_progress(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )


# ========== 3. Learning - Chat ==========

@app.post("/learn/chat/intro")
def get_subsession_intro(subsession_id: int, user_id: str = "default_user"):
    """
    ì„œë¸Œì„¸ì…˜ ì‹œì‘ ì‹œ AI íŠœí„°ì˜ ì†Œê°œ ë©”ì‹œì§€ ìƒì„±

    Args:
        subsession_id: ì„œë¸Œì„¸ì…˜ ID
        user_id: ì‚¬ìš©ì ID (ê¸°ë³¸ê°’: default_user)

    Returns:
        {
            "greeting": str,
            "topic_overview": str,
            "check_question": str,
            "full_message": str
        }
    """
    # ì„œë¸Œì„¸ì…˜ì˜ ëª¨ë“  ì²­í¬ ê°€ì ¸ì˜¤ê¸°
    chunks = vector_store.get_all_chunks_by_subsession(subsession_id)

    if not chunks:
        raise HTTPException(status_code=404, detail="No content found for this subsession")

    # ì²­í¬ ë¬¸ì„œì™€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    chunk_documents = [chunk["document"] for chunk in chunks]

    # ì„œë¸Œì„¸ì…˜ ì œëª©ê³¼ ì‚¬ìš©ì ë‹‰ë„¤ì„ ê°€ì ¸ì˜¤ê¸°
    result = db.find_subsession_by_id(subsession_id)
    if result:
        _, _, subsession = result
        subsession_title = subsession.title
    else:
        subsession_title = "í•™ìŠµ ì£¼ì œ"

    # ì‚¬ìš©ì í”„ë¡œí•„ì—ì„œ ë‹‰ë„¤ì„ ê°€ì ¸ì˜¤ê¸°
    user_profile = db.get_profile(user_id)
    user_nickname = user_profile.nickname if user_profile else "í•™ìŠµì"

    # LLMìœ¼ë¡œ ì†Œê°œ ë©”ì‹œì§€ ìƒì„±
    llm_service = get_llm_service()
    intro = llm_service.generate_subsession_intro(
        subsession_title=subsession_title,
        context_chunks=chunk_documents,
        user_nickname=user_nickname
    )

    # ì „ì²´ ë©”ì‹œì§€ ì¡°í•©
    full_message = f"{intro['greeting']}\n\n{intro['topic_overview']}\n\n{intro['check_question']}"

    return {
        "greeting": intro["greeting"],
        "topic_overview": intro["topic_overview"],
        "check_question": intro["check_question"],
        "full_message": full_message
    }


@app.post("/learn/chat", response_model=ChatResponse)
def learn_chat(request: ChatRequest):
    """
    ì±„íŒ… ê¸°ë°˜ í•™ìŠµ (ìˆœì°¨ì  ì²­í¬ ì§„í–‰)

    Args:
        request: ChatRequest (subsession_id, user_msg, chat_history, current_chunk_index)

    Returns:
        ChatResponse (explanation, prompt_to_user, covered_chunk_ids, is_complete, next_chunk_index, total_chunks)
    """
    # 1. ì„œë¸Œì„¸ì…˜ì˜ ëª¨ë“  ì²­í¬ ê°€ì ¸ì˜¤ê¸° (ìˆœì„œëŒ€ë¡œ)
    all_chunks = vector_store.get_all_chunks_by_subsession(request.subsession_id)

    if not all_chunks:
        raise HTTPException(status_code=404, detail="No content found for this subsession")

    total_chunks = len(all_chunks)
    current_index = request.current_chunk_index

    # 2. í˜„ì¬ ì²­í¬ ì„ íƒ (ì¸ë±ìŠ¤ ë²”ìœ„ í™•ì¸)
    if current_index >= total_chunks:
        # ëª¨ë“  ì²­í¬ë¥¼ ë‹¤ ë°°ì› ìœ¼ë©´ ì™„ë£Œ ë©”ì‹œì§€
        return ChatResponse(
            explanation="ëª¨ë“  ë‚´ìš©ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤! ì´ì œ í€´ì¦ˆë¡œ ë„˜ì–´ê°€ í•™ìŠµí•œ ë‚´ìš©ì„ í™•ì¸í•´ë´…ì‹œë‹¤.",
            prompt_to_user="í€´ì¦ˆë¥¼ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ?",
            covered_chunk_ids=[],
            is_complete=True,
            next_chunk_index=current_index,
            total_chunks=total_chunks
        )

    current_chunk = all_chunks[current_index]

    # 3. ë‹¤ìŒ ì²­í¬ë„ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨ (íŒíŠ¸ìš©)
    context_chunks = [current_chunk["document"]]
    if current_index + 1 < total_chunks:
        next_chunk = all_chunks[current_index + 1]
        context_chunks.append(next_chunk["document"])

    # 4. LLM ì‘ë‹µ ìƒì„±
    llm_service = get_llm_service()
    chat_history = [
        {"role": msg.role, "content": msg.content}
        for msg in request.chat_history
    ]

    response = llm_service.generate_chat_response(
        user_msg=request.user_msg,
        context_chunks=context_chunks,
        chat_history=chat_history,
        is_first_chunk=(current_index == 0),
        has_next_chunk=(current_index + 1 < total_chunks)
    )

    # 5. ì§„í–‰ ìƒíƒœ ì •ë³´ ì¶”ê°€
    response["covered_chunk_ids"] = [int(current_chunk["id"])]
    response["is_complete"] = False
    response["next_chunk_index"] = current_index + 1
    response["total_chunks"] = total_chunks

    return ChatResponse(**response)


# ========== 4. Learning - Quiz Generate ==========

@app.post("/learn/quiz:generate", response_model=QuizGenerateResponse)
def generate_quiz(request: QuizGenerateRequest):
    """
    í€´ì¦ˆ ìƒì„±

    Args:
        request: QuizGenerateRequest (subsession_id, num_questions)

    Returns:
        QuizGenerateResponse (questions)
    """
    print(f"[DEBUG] Quiz generation requested for subsession_id={request.subsession_id}, num_questions={request.num_questions}")

    # ì„œë¸Œì„¸ì…˜ì˜ ëª¨ë“  ì²­í¬ ê°€ì ¸ì˜¤ê¸°
    chunks = vector_store.get_all_chunks_by_subsession(request.subsession_id)

    if not chunks:
        raise HTTPException(status_code=404, detail="No content found for this subsession")

    # ì²­í¬ ë¬¸ì„œ ì¶”ì¶œ
    chunk_documents = [chunk["document"] for chunk in chunks]

    # LLM í€´ì¦ˆ ìƒì„±
    llm_service = get_llm_service()
    questions = llm_service.generate_quiz(
        context_chunks=chunk_documents,
        num_questions=request.num_questions
    )

    # ë””ë²„ê·¸: ìƒì„±ëœ í€´ì¦ˆ í™•ì¸
    print(f"[DEBUG] Generated {len(questions)} questions")
    for i, q in enumerate(questions):
        print(f"[DEBUG] Question {i+1}: {q.get('question', 'NO QUESTION')[:50]}...")
        print(f"[DEBUG]   - Has 'options' key: {'options' in q}")
        if 'options' in q:
            print(f"[DEBUG]   - Number of options: {len(q['options'])}")
            print(f"[DEBUG]   - Options: {q['options']}")
        print(f"[DEBUG]   - correct_answer: {q.get('correct_answer', 'MISSING')}")

    return QuizGenerateResponse(
        questions=questions,
        subsession_id=request.subsession_id
    )


# ========== 5. Learning - Quiz Submit ==========

@app.post("/learn/quiz:submit", response_model=QuizSubmitResponse)
def submit_quiz(request: QuizSubmitRequest):
    """
    í€´ì¦ˆ ì œì¶œ ë° ì±„ì  (3ì§€ì„ ë‹¤)

    Args:
        request: QuizSubmitRequest (subsession_id, answers, questions)

    Returns:
        QuizSubmitResponse (score, total, percentage, details)
    """
    results = []
    correct_count = 0

    # ë‹µë³€ ë§¤ì¹­
    answer_map = {ans.question_index: ans.selected_option for ans in request.answers}

    for i, question in enumerate(request.questions):
        selected_option = answer_map.get(i, -1)  # ì„ íƒ ì•ˆí•œ ê²½ìš° -1

        # ì •ë‹µ í™•ì¸ (ì„ íƒì§€ ì¸ë±ìŠ¤ ë¹„êµ)
        is_correct = selected_option == question.correct_answer

        if is_correct:
            correct_count += 1

        results.append(QuizResult(
            question=question.question,
            options=question.options,
            selected_option=selected_option,
            correct_answer=question.correct_answer,
            is_correct=is_correct
        ))

    total = len(request.questions)
    percentage = (correct_count / total * 100) if total > 0 else 0

    return QuizSubmitResponse(
        score=correct_count,
        total=total,
        percentage=round(percentage, 2),
        details=results
    )


# ========== 6. Learning - Summary ==========

@app.post("/learn/summary", response_model=Summary)
def generate_summary(request: SummaryRequest):
    """
    í•™ìŠµ ìš”ì•½ ìƒì„±

    Args:
        request: SummaryRequest (subsession_id)

    Returns:
        Summary (summary, pitfalls, next_topics, links)
    """
    # ì„œë¸Œì„¸ì…˜ì˜ ëª¨ë“  ì²­í¬ ê°€ì ¸ì˜¤ê¸°
    chunks = vector_store.get_all_chunks_by_subsession(request.subsession_id)

    if not chunks:
        raise HTTPException(status_code=404, detail="No content found for this subsession")

    # ì²­í¬ ë¬¸ì„œ ì¶”ì¶œ
    chunk_documents = [chunk["document"] for chunk in chunks]

    # LLM ìš”ì•½ ìƒì„±
    llm_service = get_llm_service()
    summary = llm_service.generate_summary(context_chunks=chunk_documents)

    return Summary(**summary)


# ========== 7. Subsession Complete ==========

@app.post("/subsessions/{subsession_id}/complete")
def complete_subsession(
    subsession_id: int,
    request: CompleteSubsessionRequest
):
    """
    ì„œë¸Œì„¸ì…˜ í•™ìŠµ ì™„ë£Œ ì²˜ë¦¬

    Args:
        subsession_id: ì„œë¸Œì„¸ì…˜ ID
        request: CompleteSubsessionRequest (proficiency_increase)

    Returns:
        {
            "message": str,
            "subsession_id": int,
            "new_proficiency": float,
            "new_study_count": int
        }
    """
    # ì„œë¸Œì„¸ì…˜ ì°¾ê¸° (notebook_id, session_id ëª¨ë¥´ëŠ” ìƒíƒœ)
    result = db.find_subsession_by_id(subsession_id)
    if not result:
        raise HTTPException(status_code=404, detail="Subsession not found")

    notebook_id, session_id, subsession = result

    # ìˆ™ë ¨ë„ ì—…ë°ì´íŠ¸ (100% ìƒí•œì„ )
    new_proficiency = min(100.0, subsession.proficiency + request.proficiency_increase)
    subsession.proficiency = new_proficiency

    # í•™ìŠµ íšŸìˆ˜ ì¦ê°€
    subsession.study_count += 1

    # ì„œë¸Œì„¸ì…˜ ì—…ë°ì´íŠ¸
    db.update_subsession(notebook_id, session_id, subsession)

    # ë…¸íŠ¸ë¶ í†µê³„ ì—…ë°ì´íŠ¸
    notebook = db.get_notebook(notebook_id)
    if notebook:
        # ì´ í•™ìŠµ íšŸìˆ˜ ì¦ê°€
        notebook.total_study_count += 1

        # í•™ìŠµ ì¼ìˆ˜ ì—…ë°ì´íŠ¸ (ë‚ ì§œê°€ ë°”ë€Œì—ˆëŠ”ì§€ í™•ì¸)
        today = datetime.now().date()
        if notebook.last_studied_at:
            # ê¸°ì¡´ ë§ˆì§€ë§‰ í•™ìŠµ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
            last_date = notebook.last_studied_at.date() if hasattr(notebook.last_studied_at, 'date') else datetime.fromisoformat(str(notebook.last_studied_at)).date()

            # ì˜¤ëŠ˜ ì²˜ìŒ í•™ìŠµí•˜ëŠ” ê²½ìš°
            if last_date != today:
                notebook.total_study_days += 1

                # ì—°ì† í•™ìŠµ ì¼ìˆ˜ ê³„ì‚°
                if (today - last_date).days == 1:
                    # ì–´ì œ í•™ìŠµí–ˆìœ¼ë©´ ì—°ì† ì¦ê°€
                    notebook.streak_days += 1
                else:
                    # ì¤‘ê°„ì— ëŠê²¼ìœ¼ë©´ 1ë¡œ ë¦¬ì…‹
                    notebook.streak_days = 1
        else:
            # ì²« í•™ìŠµì¸ ê²½ìš°
            notebook.total_study_days = 1
            notebook.streak_days = 1

        # ìµœê·¼ í•™ìŠµ ì •ë³´ ì—…ë°ì´íŠ¸ (ì¼ìˆ˜ ê³„ì‚° í›„ì— ì—…ë°ì´íŠ¸)
        notebook.last_studied_at = datetime.now()
        notebook.last_session_title = subsession.title

        # ë…¸íŠ¸ë¶ ì €ì¥
        db.update_notebook(notebook)

    return {
        "message": "Learning session completed successfully",
        "subsession_id": subsession_id,
        "new_proficiency": new_proficiency,
        "new_study_count": subsession.study_count
    }


# ========== 8. User Profile ==========

@app.post("/profile", response_model=UserProfile)
def create_profile(profile: UserProfile):
    """í”„ë¡œí•„ ìƒì„±/ì—…ë°ì´íŠ¸"""
    db.save_profile(profile)
    return profile


@app.get("/profile", response_model=Optional[UserProfile])
def get_profile(user_id: str = "default_user"):
    """í”„ë¡œí•„ ì¡°íšŒ"""
    profile = db.get_profile(user_id)
    return profile


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
