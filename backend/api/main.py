from fastapi import FastAPI, UploadFile, File, HTTPException, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from typing import List, Optional, AsyncGenerator
import os
import uuid
from datetime import datetime
import json
import asyncio

from ..models import (
    Notebook,
    Session,
    Subsession,
    UserProfile,
    SessionStatus,
    ChatRequest,
    ChatResponse,
    QuizGenerateRequest,
    QuizGenerateResponse,
    QuizSubmitRequest,
    QuizSubmitResponse,
    QuizResult,
    SummaryRequest,
    Summary,
    CompleteSubsessionRequest,
)
from ..services import (
    VectorStoreService,
    EmbeddingService,
    LLMService,
    DocumentProcessor,
    get_embedding_service,
    get_llm_service,
)
from ..utils import get_database

# FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI(
    title="TalkiLearn API",
    description="í•™ìŠµë³´ì¡° ì±—ë´‡ ë°±ì—”ë“œ API",
    version="1.0.0"
)

# CORS ì„¤ì • (Streamlit í”„ë¡ íŠ¸ì—”ë“œì™€ í†µì‹ )
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ê°œë°œìš© - í”„ë¡œë•ì…˜ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ (ì „ì—­)
vector_store = VectorStoreService()
db = get_database()


@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
    print("ğŸš€ TalkiLearn API starting up...")
    print("ğŸ“¦ Loading embedding model...")
    # ì„ë² ë”© ëª¨ë¸ ì‚¬ì „ ë¡œë“œ
    get_embedding_service()
    print("âœ… Embedding model loaded successfully")
    print("ğŸ¤– LLM service initialized")
    get_llm_service()
    print("âœ… TalkiLearn API ready!")


# ========== Health Check ==========

@app.get("/")
async def root():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "service": "TalkiLearn API",
        "version": "1.0.0"
    }


# ========== 1. Notebooks ==========

@app.post("/notebooks", response_model=Notebook)
async def create_notebook(title: str, user_id: str = "default_user"):
    """
    ë…¸íŠ¸ë¶ ìƒì„±

    Args:
        title: ê³¼ëª©ëª…
        user_id: ì‚¬ìš©ì ID
    """
    notebook = db.create_notebook(title=title, user_id=user_id)
    return notebook


@app.get("/notebooks", response_model=List[Notebook])
async def get_notebooks(user_id: str = "default_user"):
    """
    ë…¸íŠ¸ë¶ ëª©ë¡ ì¡°íšŒ

    Args:
        user_id: ì‚¬ìš©ì ID
    """
    notebooks = db.get_all_notebooks(user_id=user_id)
    return notebooks


@app.get("/notebooks/{notebook_id}", response_model=Notebook)
async def get_notebook(notebook_id: int):
    """
    ë…¸íŠ¸ë¶ ìƒì„¸ ì¡°íšŒ

    Args:
        notebook_id: ë…¸íŠ¸ë¶ ID
    """
    notebook = db.get_notebook(notebook_id)
    if not notebook:
        raise HTTPException(status_code=404, detail="Notebook not found")
    return notebook


@app.delete("/notebooks/{notebook_id}")
async def delete_notebook(notebook_id: int):
    """
    ë…¸íŠ¸ë¶ ì‚­ì œ

    Args:
        notebook_id: ë…¸íŠ¸ë¶ ID
    """
    notebook = db.get_notebook(notebook_id)
    if not notebook:
        raise HTTPException(status_code=404, detail="Notebook not found")

    # ë…¸íŠ¸ë¶ì˜ ëª¨ë“  ì„¸ì…˜ì— ëŒ€í•œ ë²¡í„° ìŠ¤í† ì–´ ì²­í¬ ì‚­ì œ
    for session in notebook.sessions:
        try:
            vector_store.delete_session_chunks(session.session_id)
        except Exception as e:
            print(f"Warning: Failed to delete chunks for session {session.session_id}: {e}")

    # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë…¸íŠ¸ë¶ ì‚­ì œ
    success = db.delete_notebook(notebook_id)
    if not success:
        raise HTTPException(status_code=500, detail="Failed to delete notebook")

    return {"message": "Notebook deleted successfully", "notebook_id": notebook_id}


# ========== 2. Session Upload ==========

@app.post("/sessions:upload")
async def upload_session(
    notebook_id: int = Form(...),
    file: UploadFile = File(...)
):
    """
    íŒŒì¼ ì—…ë¡œë“œ ë° ì„¸ì…˜ ìƒì„±

    1. íŒŒì¼ íŒŒì‹± (txt/pdf)
    2. ì²­í‚¹ (size=800, overlap=0.2)
    3. ì„ë² ë”© ìƒì„±
    4. Chromaì— ì €ì¥
    5. í† í”½ í´ëŸ¬ìŠ¤í„°ë§ â†’ Subsession ë¶„í• 
    6. ì„¸ì…˜ ìƒíƒœë¥¼ indexedë¡œ ë³€ê²½

    Args:
        notebook_id: ë…¸íŠ¸ë¶ ID
        file: ì—…ë¡œë“œ íŒŒì¼
    """
    # ë…¸íŠ¸ë¶ ì¡´ì¬ í™•ì¸
    notebook = db.get_notebook(notebook_id)
    if not notebook:
        raise HTTPException(status_code=404, detail="Notebook not found")

    # íŒŒì¼ íƒ€ì… í™•ì¸
    filename = file.filename
    file_type = filename.split(".")[-1].lower()
    if file_type not in ["txt", "pdf"]:
        raise HTTPException(status_code=400, detail="Unsupported file type. Only txt and pdf are allowed.")

    # ì„¸ì…˜ ìƒì„±
    max_session_id = max([s.session_id for s in notebook.sessions], default=0)
    new_session_id = max_session_id + 1

    session = Session(
        session_id=new_session_id,
        notebook_id=notebook_id,
        filename=filename,
        file_type=file_type,
        status=SessionStatus.PROCESSING
    )

    db.add_session_to_notebook(notebook_id, session)

    try:
        # 1. íŒŒì¼ ì½ê¸°
        file_bytes = await file.read()

        # 2. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        from ..services.document_processor import extract_text_from_bytes
        text = extract_text_from_bytes(file_bytes, file_type)

        # 3. ì²­í‚¹
        processor = DocumentProcessor(chunk_size=800, overlap_ratio=0.2)
        chunks = processor.chunk_text(text)
        session.total_chunks = len(chunks)

        # 4. ì„ë² ë”© ìƒì„±
        embedding_service = get_embedding_service()
        embeddings = embedding_service.encode(chunks, show_progress=False)

        # 5. í† í”½ í´ëŸ¬ìŠ¤í„°ë§
        cluster_labels, num_clusters = processor.cluster_chunks_into_subsessions(
            embeddings,
            min_clusters=3,
            max_clusters=8
        )

        # 6. Subsession ìƒì„±
        subsessions = []
        # ê³ ìœ  ID ìƒì„±: notebook_idì™€ session_idë¥¼ ì¡°í•©í•˜ì—¬ ì „ì—­ì ìœ¼ë¡œ ê³ ìœ í•˜ê²Œ ë§Œë“¦
        subsession_id_base = notebook_id * 100000 + new_session_id * 1000

        for cluster_idx in range(num_clusters):
            # í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì˜ ì²­í¬ ì¸ë±ìŠ¤ ì°¾ê¸°
            cluster_chunk_indices = [i for i, label in enumerate(cluster_labels) if label == cluster_idx]

            # ì²­í¬ ID ë¦¬ìŠ¤íŠ¸
            chunk_ids = list(range(subsession_id_base + cluster_idx * 100, subsession_id_base + cluster_idx * 100 + len(cluster_chunk_indices)))

            # ì„œë¸Œì„¸ì…˜ ì œëª© ìƒì„± (LLM ì‚¬ìš©)
            cluster_chunks = [chunks[i] for i in cluster_chunk_indices]
            llm_service = get_llm_service()
            title = llm_service.generate_subsession_title(cluster_chunks)

            subsession = Subsession(
                subsession_id=subsession_id_base + cluster_idx,
                session_id=new_session_id,
                index=cluster_idx + 1,
                title=title,
                chunk_ids=chunk_ids
            )
            subsessions.append(subsession)

            # 7. Chromaì— ì²­í¬ ì €ì¥
            chunk_embeddings = [embeddings[i] for i in cluster_chunk_indices]
            chunk_documents = cluster_chunks
            chunk_metadatas = [
                {
                    "notebook_id": notebook_id,
                    "session_id": new_session_id,
                    "subsession_id": subsession.subsession_id,
                    "chunk_id": chunk_id,
                    "cluster": cluster_idx
                }
                for chunk_id in chunk_ids
            ]
            chunk_id_strings = [str(chunk_id) for chunk_id in chunk_ids]

            vector_store.add_chunks(
                chunks=chunk_documents,
                embeddings=chunk_embeddings,
                metadatas=chunk_metadatas,
                chunk_ids=chunk_id_strings
            )

        # 8. ì„¸ì…˜ ì—…ë°ì´íŠ¸
        session.subsessions = subsessions
        session.status = SessionStatus.INDEXED
        session.indexed_at = datetime.now()

        db.update_session(notebook_id, session)

        return {
            "message": "File uploaded and processed successfully",
            "session_id": new_session_id,
            "total_chunks": len(chunks),
            "num_subsessions": num_clusters,
            "subsessions": [
                {
                    "subsession_id": sub.subsession_id,
                    "index": sub.index,
                    "title": sub.title,
                    "num_chunks": len(sub.chunk_ids)
                }
                for sub in subsessions
            ]
        }

    except Exception as e:
        # ì—ëŸ¬ ë°œìƒ ì‹œ ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        session.status = SessionStatus.ERROR
        db.update_session(notebook_id, session)
        raise HTTPException(status_code=500, detail=f"Error processing file: {str(e)}")


@app.post("/sessions:upload-stream")
async def upload_session_with_progress(
    notebook_id: int = Form(...),
    file: UploadFile = File(...)
):
    """
    íŒŒì¼ ì—…ë¡œë“œ ë° ì„¸ì…˜ ìƒì„± (ì§„í–‰ë¥  ìŠ¤íŠ¸ë¦¬ë°)

    Server-Sent Events (SSE) í˜•ì‹ìœ¼ë¡œ ì§„í–‰ë¥ ì„ ì‹¤ì‹œê°„ ì „ì†¡í•©ë‹ˆë‹¤.
    """
    async def generate_progress() -> AsyncGenerator[str, None]:
        try:
            # ë…¸íŠ¸ë¶ ì¡´ì¬ í™•ì¸
            notebook = db.get_notebook(notebook_id)
            if not notebook:
                yield f"data: {json.dumps({'error': 'Notebook not found'})}\n\n"
                return

            # íŒŒì¼ íƒ€ì… í™•ì¸
            filename = file.filename
            file_type = filename.split(".")[-1].lower()
            if file_type not in ["txt", "pdf"]:
                yield f"data: {json.dumps({'error': 'Unsupported file type'})}\n\n"
                return

            yield f"data: {json.dumps({'stage': 'init', 'message': 'íŒŒì¼ ì—…ë¡œë“œ ì‹œì‘...', 'progress': 0})}\n\n"
            await asyncio.sleep(0.1)

            # ì„¸ì…˜ ìƒì„±
            max_session_id = max([s.session_id for s in notebook.sessions], default=0)
            new_session_id = max_session_id + 1

            session = Session(
                session_id=new_session_id,
                notebook_id=notebook_id,
                filename=filename,
                file_type=file_type,
                status=SessionStatus.PROCESSING
            )
            db.add_session_to_notebook(notebook_id, session)

            # 1. íŒŒì¼ ì½ê¸°
            yield f"data: {json.dumps({'stage': 'reading', 'message': 'íŒŒì¼ ì½ëŠ” ì¤‘...', 'progress': 10})}\n\n"
            file_bytes = await file.read()

            # 2. í…ìŠ¤íŠ¸ ì¶”ì¶œ
            yield f"data: {json.dumps({'stage': 'extracting', 'message': 'í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...', 'progress': 20})}\n\n"
            from ..services.document_processor import extract_text_from_bytes
            text = extract_text_from_bytes(file_bytes, file_type)

            # 3. ì²­í‚¹
            yield f"data: {json.dumps({'stage': 'chunking', 'message': 'í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘...', 'progress': 30})}\n\n"
            processor = DocumentProcessor(chunk_size=800, overlap_ratio=0.2)
            chunks = processor.chunk_text(text)
            session.total_chunks = len(chunks)

            # 4. ì„ë² ë”© ìƒì„± (ì§„í–‰ë¥  ì½œë°± ì‚¬ìš©)
            embedding_service = get_embedding_service()
            embeddings_list = []

            def embedding_progress(current: int, total: int):
                progress = 30 + int((current / total) * 40)  # 30-70%
                embeddings_list.clear()
                embeddings_list.append((current, total))

            yield f"data: {json.dumps({'stage': 'embedding', 'message': f'ì„ë² ë”© ìƒì„± ì¤‘ (0/{len(chunks)})...', 'progress': 30})}\n\n"

            # ì§„í–‰ë¥ ì„ ë³´ê³ í•˜ë©´ì„œ ì„ë² ë”© ìƒì„±
            embeddings = embedding_service.encode(
                chunks,
                show_progress=False,
                progress_callback=embedding_progress
            )

            # ì„ë² ë”© ì™„ë£Œ í›„ ì§„í–‰ë¥  ì „ì†¡
            if embeddings_list:
                current, total = embeddings_list[0]
                yield f"data: {json.dumps({'stage': 'embedding', 'message': f'ì„ë² ë”© ìƒì„± ì™„ë£Œ ({total}/{total})', 'progress': 70})}\n\n"

            # 5. í† í”½ í´ëŸ¬ìŠ¤í„°ë§
            yield f"data: {json.dumps({'stage': 'clustering', 'message': 'í† í”½ í´ëŸ¬ìŠ¤í„°ë§ ì¤‘...', 'progress': 75})}\n\n"
            cluster_labels, num_clusters = processor.cluster_chunks_into_subsessions(
                embeddings,
                min_clusters=3,
                max_clusters=8
            )

            # 6. Subsession ìƒì„±
            yield f"data: {json.dumps({'stage': 'creating_subsessions', 'message': 'ì„œë¸Œì„¸ì…˜ ìƒì„± ì¤‘...', 'progress': 80})}\n\n"
            subsessions = []
            subsession_id_base = notebook_id * 100000 + new_session_id * 1000

            for cluster_idx in range(num_clusters):
                cluster_chunk_indices = [i for i, label in enumerate(cluster_labels) if label == cluster_idx]
                chunk_ids = list(range(subsession_id_base + cluster_idx * 100, subsession_id_base + cluster_idx * 100 + len(cluster_chunk_indices)))

                # ì„œë¸Œì„¸ì…˜ ì œëª© ìƒì„±
                cluster_chunks = [chunks[i] for i in cluster_chunk_indices]
                llm_service = get_llm_service()
                title = llm_service.generate_subsession_title(cluster_chunks)

                subsession = Subsession(
                    subsession_id=subsession_id_base + cluster_idx,
                    session_id=new_session_id,
                    index=cluster_idx + 1,
                    title=title,
                    chunk_ids=chunk_ids
                )
                subsessions.append(subsession)

                # 7. Chromaì— ì²­í¬ ì €ì¥
                chunk_embeddings = [embeddings[i] for i in cluster_chunk_indices]
                chunk_documents = cluster_chunks
                chunk_metadatas = [
                    {
                        "notebook_id": notebook_id,
                        "session_id": new_session_id,
                        "subsession_id": subsession.subsession_id,
                        "chunk_id": chunk_id,
                        "cluster": cluster_idx
                    }
                    for chunk_id in chunk_ids
                ]
                chunk_id_strings = [str(chunk_id) for chunk_id in chunk_ids]

                vector_store.add_chunks(
                    chunks=chunk_documents,
                    embeddings=chunk_embeddings,
                    metadatas=chunk_metadatas,
                    chunk_ids=chunk_id_strings
                )

                progress = 80 + int(((cluster_idx + 1) / num_clusters) * 15)  # 80-95%
                yield f"data: {json.dumps({'stage': 'saving', 'message': f'ì„œë¸Œì„¸ì…˜ ì €ì¥ ì¤‘ ({cluster_idx+1}/{num_clusters})...', 'progress': progress})}\n\n"

            # 8. ì„¸ì…˜ ì—…ë°ì´íŠ¸
            session.subsessions = subsessions
            session.status = SessionStatus.INDEXED
            session.indexed_at = datetime.now()
            db.update_session(notebook_id, session)

            # ì™„ë£Œ
            result = {
                "stage": "complete",
                "message": "íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!",
                "progress": 100,
                "session_id": new_session_id,
                "total_chunks": len(chunks),
                "num_subsessions": num_clusters,
                "subsessions": [
                    {
                        "subsession_id": sub.subsession_id,
                        "index": sub.index,
                        "title": sub.title,
                        "num_chunks": len(sub.chunk_ids)
                    }
                    for sub in subsessions
                ]
            }
            yield f"data: {json.dumps(result)}\n\n"

        except Exception as e:
            # ì—ëŸ¬ ë°œìƒ ì‹œ
            if 'session' in locals():
                session.status = SessionStatus.ERROR
                db.update_session(notebook_id, session)

            error_data = {
                "stage": "error",
                "message": f"ì—ëŸ¬ ë°œìƒ: {str(e)}",
                "progress": 0
            }
            yield f"data: {json.dumps(error_data)}\n\n"

    return StreamingResponse(
        generate_progress(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )


# ========== 3. Learning - Chat ==========

@app.post("/learn/chat/intro")
async def get_subsession_intro(subsession_id: int, user_id: str = "default_user"):
    """
    ì„œë¸Œì„¸ì…˜ ì‹œì‘ ì‹œ AI íŠœí„°ì˜ ì†Œê°œ ë©”ì‹œì§€ ìƒì„±

    Args:
        subsession_id: ì„œë¸Œì„¸ì…˜ ID
        user_id: ì‚¬ìš©ì ID (ê¸°ë³¸ê°’: default_user)

    Returns:
        {
            "greeting": str,
            "topic_overview": str,
            "check_question": str,
            "full_message": str
        }
    """
    # ì„œë¸Œì„¸ì…˜ì˜ ëª¨ë“  ì²­í¬ ê°€ì ¸ì˜¤ê¸°
    chunks = vector_store.get_all_chunks_by_subsession(subsession_id)

    if not chunks:
        raise HTTPException(status_code=404, detail="No content found for this subsession")

    # ì²­í¬ ë¬¸ì„œì™€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    chunk_documents = [chunk["document"] for chunk in chunks]

    # ì„œë¸Œì„¸ì…˜ ì œëª©ê³¼ ì‚¬ìš©ì ë‹‰ë„¤ì„ ê°€ì ¸ì˜¤ê¸°
    result = db.find_subsession_by_id(subsession_id)
    if result:
        _, _, subsession = result
        subsession_title = subsession.title
    else:
        subsession_title = "í•™ìŠµ ì£¼ì œ"

    # ì‚¬ìš©ì í”„ë¡œí•„ì—ì„œ ë‹‰ë„¤ì„ ê°€ì ¸ì˜¤ê¸°
    user_profile = db.get_profile(user_id)
    user_nickname = user_profile.nickname if user_profile else "í•™ìŠµì"

    # LLMìœ¼ë¡œ ì†Œê°œ ë©”ì‹œì§€ ìƒì„±
    llm_service = get_llm_service()
    intro = llm_service.generate_subsession_intro(
        subsession_title=subsession_title,
        context_chunks=chunk_documents,
        user_nickname=user_nickname
    )

    # ì „ì²´ ë©”ì‹œì§€ ì¡°í•©
    full_message = f"{intro['greeting']}\n\n{intro['topic_overview']}\n\n{intro['check_question']}"

    return {
        "greeting": intro["greeting"],
        "topic_overview": intro["topic_overview"],
        "check_question": intro["check_question"],
        "full_message": full_message
    }


@app.post("/learn/chat", response_model=ChatResponse)
async def learn_chat(request: ChatRequest):
    """
    ì±„íŒ… ê¸°ë°˜ í•™ìŠµ

    Args:
        request: ChatRequest (subsession_id, user_msg, chat_history)

    Returns:
        ChatResponse (explanation, prompt_to_user, covered_chunk_ids)
    """
    # 1. ì‚¬ìš©ì ë©”ì‹œì§€ ì„ë² ë”©
    embedding_service = get_embedding_service()
    query_embedding = embedding_service.encode_query(request.user_msg)

    # 2. ë²¡í„° ê²€ìƒ‰ (Top-K=6)
    results = vector_store.query_chunks(
        query_embedding=query_embedding,
        subsession_id=request.subsession_id,
        top_k=6
    )

    if not results["documents"]:
        raise HTTPException(status_code=404, detail="No content found for this subsession")

    # 3. LLM ì‘ë‹µ ìƒì„±
    llm_service = get_llm_service()
    chat_history = [
        {"role": msg.role, "content": msg.content}
        for msg in request.chat_history
    ]

    response = llm_service.generate_chat_response(
        user_msg=request.user_msg,
        context_chunks=results["documents"],
        chat_history=chat_history
    )

    # covered_chunk_idsëŠ” ê²€ìƒ‰ëœ ì²­í¬ì˜ ID
    covered_ids = [int(id_str) for id_str in results["ids"]]
    response["covered_chunk_ids"] = covered_ids

    return ChatResponse(**response)


# ========== 4. Learning - Quiz Generate ==========

@app.post("/learn/quiz:generate", response_model=QuizGenerateResponse)
async def generate_quiz(request: QuizGenerateRequest):
    """
    í€´ì¦ˆ ìƒì„±

    Args:
        request: QuizGenerateRequest (subsession_id, num_questions)

    Returns:
        QuizGenerateResponse (questions)
    """
    print(f"[DEBUG] Quiz generation requested for subsession_id={request.subsession_id}, num_questions={request.num_questions}")

    # ì„œë¸Œì„¸ì…˜ì˜ ëª¨ë“  ì²­í¬ ê°€ì ¸ì˜¤ê¸°
    chunks = vector_store.get_all_chunks_by_subsession(request.subsession_id)

    if not chunks:
        raise HTTPException(status_code=404, detail="No content found for this subsession")

    # ì²­í¬ ë¬¸ì„œ ì¶”ì¶œ
    chunk_documents = [chunk["document"] for chunk in chunks]

    # LLM í€´ì¦ˆ ìƒì„±
    llm_service = get_llm_service()
    questions = llm_service.generate_quiz(
        context_chunks=chunk_documents,
        num_questions=request.num_questions
    )

    # ë””ë²„ê·¸: ìƒì„±ëœ í€´ì¦ˆ í™•ì¸
    print(f"[DEBUG] Generated {len(questions)} questions")
    for i, q in enumerate(questions):
        print(f"[DEBUG] Question {i+1}: {q.get('question', 'NO QUESTION')[:50]}...")
        print(f"[DEBUG]   - Has 'options' key: {'options' in q}")
        if 'options' in q:
            print(f"[DEBUG]   - Number of options: {len(q['options'])}")
            print(f"[DEBUG]   - Options: {q['options']}")
        print(f"[DEBUG]   - correct_answer: {q.get('correct_answer', 'MISSING')}")

    return QuizGenerateResponse(
        questions=questions,
        subsession_id=request.subsession_id
    )


# ========== 5. Learning - Quiz Submit ==========

@app.post("/learn/quiz:submit", response_model=QuizSubmitResponse)
async def submit_quiz(request: QuizSubmitRequest):
    """
    í€´ì¦ˆ ì œì¶œ ë° ì±„ì  (3ì§€ì„ ë‹¤)

    Args:
        request: QuizSubmitRequest (subsession_id, answers, questions)

    Returns:
        QuizSubmitResponse (score, total, percentage, details)
    """
    results = []
    correct_count = 0

    # ë‹µë³€ ë§¤ì¹­
    answer_map = {ans.question_index: ans.selected_option for ans in request.answers}

    for i, question in enumerate(request.questions):
        selected_option = answer_map.get(i, -1)  # ì„ íƒ ì•ˆí•œ ê²½ìš° -1

        # ì •ë‹µ í™•ì¸ (ì„ íƒì§€ ì¸ë±ìŠ¤ ë¹„êµ)
        is_correct = selected_option == question.correct_answer

        if is_correct:
            correct_count += 1

        results.append(QuizResult(
            question=question.question,
            options=question.options,
            selected_option=selected_option,
            correct_answer=question.correct_answer,
            is_correct=is_correct
        ))

    total = len(request.questions)
    percentage = (correct_count / total * 100) if total > 0 else 0

    return QuizSubmitResponse(
        score=correct_count,
        total=total,
        percentage=round(percentage, 2),
        details=results
    )


# ========== 6. Learning - Summary ==========

@app.post("/learn/summary", response_model=Summary)
async def generate_summary(request: SummaryRequest):
    """
    í•™ìŠµ ìš”ì•½ ìƒì„±

    Args:
        request: SummaryRequest (subsession_id)

    Returns:
        Summary (summary, pitfalls, next_topics, links)
    """
    # ì„œë¸Œì„¸ì…˜ì˜ ëª¨ë“  ì²­í¬ ê°€ì ¸ì˜¤ê¸°
    chunks = vector_store.get_all_chunks_by_subsession(request.subsession_id)

    if not chunks:
        raise HTTPException(status_code=404, detail="No content found for this subsession")

    # ì²­í¬ ë¬¸ì„œ ì¶”ì¶œ
    chunk_documents = [chunk["document"] for chunk in chunks]

    # LLM ìš”ì•½ ìƒì„±
    llm_service = get_llm_service()
    summary = llm_service.generate_summary(context_chunks=chunk_documents)

    return Summary(**summary)


# ========== 7. Subsession Complete ==========

@app.post("/subsessions/{subsession_id}/complete")
async def complete_subsession(
    subsession_id: int,
    request: CompleteSubsessionRequest
):
    """
    ì„œë¸Œì„¸ì…˜ í•™ìŠµ ì™„ë£Œ ì²˜ë¦¬

    Args:
        subsession_id: ì„œë¸Œì„¸ì…˜ ID
        request: CompleteSubsessionRequest (proficiency_increase)

    Returns:
        {
            "message": str,
            "subsession_id": int,
            "new_proficiency": float,
            "new_study_count": int
        }
    """
    # ì„œë¸Œì„¸ì…˜ ì°¾ê¸° (notebook_id, session_id ëª¨ë¥´ëŠ” ìƒíƒœ)
    result = db.find_subsession_by_id(subsession_id)
    if not result:
        raise HTTPException(status_code=404, detail="Subsession not found")

    notebook_id, session_id, subsession = result

    # ìˆ™ë ¨ë„ ì—…ë°ì´íŠ¸ (100% ìƒí•œì„ )
    new_proficiency = min(100.0, subsession.proficiency + request.proficiency_increase)
    subsession.proficiency = new_proficiency

    # í•™ìŠµ íšŸìˆ˜ ì¦ê°€
    subsession.study_count += 1

    # ì„œë¸Œì„¸ì…˜ ì—…ë°ì´íŠ¸
    db.update_subsession(notebook_id, session_id, subsession)

    # ë…¸íŠ¸ë¶ í†µê³„ ì—…ë°ì´íŠ¸
    notebook = db.get_notebook(notebook_id)
    if notebook:
        # ì´ í•™ìŠµ íšŸìˆ˜ ì¦ê°€
        notebook.total_study_count += 1

        # í•™ìŠµ ì¼ìˆ˜ ì—…ë°ì´íŠ¸ (ë‚ ì§œê°€ ë°”ë€Œì—ˆëŠ”ì§€ í™•ì¸)
        today = datetime.now().date()
        if notebook.last_studied_at:
            # ê¸°ì¡´ ë§ˆì§€ë§‰ í•™ìŠµ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
            last_date = notebook.last_studied_at.date() if hasattr(notebook.last_studied_at, 'date') else datetime.fromisoformat(str(notebook.last_studied_at)).date()

            # ì˜¤ëŠ˜ ì²˜ìŒ í•™ìŠµí•˜ëŠ” ê²½ìš°
            if last_date != today:
                notebook.total_study_days += 1

                # ì—°ì† í•™ìŠµ ì¼ìˆ˜ ê³„ì‚°
                if (today - last_date).days == 1:
                    # ì–´ì œ í•™ìŠµí–ˆìœ¼ë©´ ì—°ì† ì¦ê°€
                    notebook.streak_days += 1
                else:
                    # ì¤‘ê°„ì— ëŠê²¼ìœ¼ë©´ 1ë¡œ ë¦¬ì…‹
                    notebook.streak_days = 1
        else:
            # ì²« í•™ìŠµì¸ ê²½ìš°
            notebook.total_study_days = 1
            notebook.streak_days = 1

        # ìµœê·¼ í•™ìŠµ ì •ë³´ ì—…ë°ì´íŠ¸ (ì¼ìˆ˜ ê³„ì‚° í›„ì— ì—…ë°ì´íŠ¸)
        notebook.last_studied_at = datetime.now()
        notebook.last_session_title = subsession.title

        # ë…¸íŠ¸ë¶ ì €ì¥
        db.update_notebook(notebook)

    return {
        "message": "Learning session completed successfully",
        "subsession_id": subsession_id,
        "new_proficiency": new_proficiency,
        "new_study_count": subsession.study_count
    }


# ========== 8. User Profile ==========

@app.post("/profile", response_model=UserProfile)
async def create_profile(profile: UserProfile):
    """í”„ë¡œí•„ ìƒì„±/ì—…ë°ì´íŠ¸"""
    db.save_profile(profile)
    return profile


@app.get("/profile", response_model=Optional[UserProfile])
async def get_profile(user_id: str = "default_user"):
    """í”„ë¡œí•„ ì¡°íšŒ"""
    profile = db.get_profile(user_id)
    return profile


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
