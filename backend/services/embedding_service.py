from sentence_transformers import SentenceTransformer
from typing import List, Callable, Optional
import numpy as np
import torch


class EmbeddingService:
    """ì„ë² ë”© ìƒì„± ì„œë¹„ìŠ¤ (sentence-transformers ì‚¬ìš©)"""

    def __init__(self, model_name: str = "paraphrase-MiniLM-L3-v2"):
        """
        Args:
            model_name: sentence-transformers ëª¨ë¸ëª…
                       ê¸°ë³¸ê°’: paraphrase-MiniLM-L3-v2 (ë§¤ìš° ë¹ ë¥¸ ê²½ëŸ‰ ëª¨ë¸, 2-3ë°° ì†ë„ í–¥ìƒ)
        """
        self.model_name = model_name

        # GPU ë””ë°”ì´ìŠ¤ ì„¤ì • (Apple Silicon MPS ë˜ëŠ” CUDA)
        if torch.backends.mps.is_available():
            self.device = "mps"
            print(f"ğŸš€ Using Apple Silicon GPU (MPS) for acceleration")
        elif torch.cuda.is_available():
            self.device = "cuda"
            print(f"ğŸš€ Using CUDA GPU for acceleration")
        else:
            self.device = "cpu"
            print(f"âš ï¸ Using CPU (no GPU available)")

        print(f"Loading embedding model: {model_name}...")
        self.model = SentenceTransformer(model_name, device=self.device)
        print(f"Embedding model loaded successfully on {self.device.upper()}")

    def encode(
        self,
        texts: List[str],
        batch_size: int = 64,  # ë°°ì¹˜ í¬ê¸° ì¦ê°€ (32 -> 64)
        show_progress: bool = False,
        progress_callback: Optional[Callable[[int, int], None]] = None
    ) -> List[List[float]]:
        """
        í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜

        Args:
            texts: í…ìŠ¤íŠ¸ ëª©ë¡
            batch_size: ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’ 64ë¡œ ì¦ê°€)
            show_progress: ì§„í–‰ë¥  í‘œì‹œ ì—¬ë¶€
            progress_callback: ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜ (current, total)

        Returns:
            ì„ë² ë”© ë²¡í„° ëª©ë¡ (L2 ì •ê·œí™” ì ìš©)
        """
        if progress_callback:
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ë©° ì§„í–‰ë¥  ë³´ê³ 
            all_embeddings = []
            total_texts = len(texts)

            for i in range(0, total_texts, batch_size):
                batch = texts[i:i + batch_size]
                batch_embeddings = self.model.encode(
                    batch,
                    batch_size=batch_size,
                    show_progress_bar=False,
                    normalize_embeddings=True,
                    convert_to_numpy=True
                )
                all_embeddings.extend(batch_embeddings)

                # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ
                progress_callback(min(i + batch_size, total_texts), total_texts)

            return np.array(all_embeddings).tolist()
        else:
            # ê¸°ì¡´ ë°©ì‹ (ì§„í–‰ë¥  ì½œë°± ì—†ìŒ)
            embeddings = self.model.encode(
                texts,
                batch_size=batch_size,
                show_progress_bar=show_progress,
                normalize_embeddings=True,  # L2 normalization
                convert_to_numpy=True
            )
            return embeddings.tolist()

    def encode_query(self, query: str) -> List[float]:
        """
        ë‹¨ì¼ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜

        Args:
            query: ì¿¼ë¦¬ í…ìŠ¤íŠ¸

        Returns:
            ì„ë² ë”© ë²¡í„°
        """
        embedding = self.model.encode(
            query,
            normalize_embeddings=True,
            convert_to_numpy=True
        )
        return embedding.tolist()

    def get_embedding_dimension(self) -> int:
        """ì„ë² ë”© ì°¨ì› ë°˜í™˜"""
        return self.model.get_sentence_embedding_dimension()


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ (ì „ì—­ì—ì„œ ì¬ì‚¬ìš©)
_embedding_service = None


def get_embedding_service() -> EmbeddingService:
    """ì„ë² ë”© ì„œë¹„ìŠ¤ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _embedding_service
    if _embedding_service is None:
        _embedding_service = EmbeddingService()
    return _embedding_service
